{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:57:04.063415Z",
     "start_time": "2020-02-02T15:57:03.322130Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>power (MW)</th>\n",
       "      <th>wind direction at 100m (deg)</th>\n",
       "      <th>wind speed at 100m (m/s)</th>\n",
       "      <th>air temperature at 2m (K)</th>\n",
       "      <th>surface air pressure (Pa)</th>\n",
       "      <th>density at hub height (kg/m^3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>105408.0</td>\n",
       "      <td>105408.000000</td>\n",
       "      <td>105408.000000</td>\n",
       "      <td>105408.000000</td>\n",
       "      <td>105408.000000</td>\n",
       "      <td>105408.000000</td>\n",
       "      <td>105408.000000</td>\n",
       "      <td>105408.000000</td>\n",
       "      <td>105408.000000</td>\n",
       "      <td>105408.000000</td>\n",
       "      <td>105408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>6.513661</td>\n",
       "      <td>15.756831</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>4.125135</td>\n",
       "      <td>193.870912</td>\n",
       "      <td>7.899902</td>\n",
       "      <td>287.464005</td>\n",
       "      <td>84606.872893</td>\n",
       "      <td>1.011686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.451250</td>\n",
       "      <td>8.811571</td>\n",
       "      <td>6.922219</td>\n",
       "      <td>17.260345</td>\n",
       "      <td>3.694119</td>\n",
       "      <td>88.377513</td>\n",
       "      <td>4.100500</td>\n",
       "      <td>9.776362</td>\n",
       "      <td>468.523564</td>\n",
       "      <td>0.033038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>264.279000</td>\n",
       "      <td>83157.784000</td>\n",
       "      <td>0.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>0.647000</td>\n",
       "      <td>122.959750</td>\n",
       "      <td>4.797000</td>\n",
       "      <td>279.383000</td>\n",
       "      <td>84290.328000</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>2.985500</td>\n",
       "      <td>212.370500</td>\n",
       "      <td>7.562000</td>\n",
       "      <td>287.852000</td>\n",
       "      <td>84634.568000</td>\n",
       "      <td>1.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>41.250000</td>\n",
       "      <td>7.813000</td>\n",
       "      <td>267.541000</td>\n",
       "      <td>10.546250</td>\n",
       "      <td>295.039000</td>\n",
       "      <td>84930.296000</td>\n",
       "      <td>1.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>359.989000</td>\n",
       "      <td>33.057000</td>\n",
       "      <td>309.620000</td>\n",
       "      <td>86287.160000</td>\n",
       "      <td>1.102000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Year          Month            Day           Hour         Minute  \\\n",
       "count  105408.0  105408.000000  105408.000000  105408.000000  105408.000000   \n",
       "mean     2012.0       6.513661      15.756831      11.500000      27.500000   \n",
       "std         0.0       3.451250       8.811571       6.922219      17.260345   \n",
       "min      2012.0       1.000000       1.000000       0.000000       0.000000   \n",
       "25%      2012.0       4.000000       8.000000       5.750000      13.750000   \n",
       "50%      2012.0       7.000000      16.000000      11.500000      27.500000   \n",
       "75%      2012.0      10.000000      23.000000      17.250000      41.250000   \n",
       "max      2012.0      12.000000      31.000000      23.000000      55.000000   \n",
       "\n",
       "          power (MW)  wind direction at 100m (deg)  wind speed at 100m (m/s)  \\\n",
       "count  105408.000000                 105408.000000             105408.000000   \n",
       "mean        4.125135                    193.870912                  7.899902   \n",
       "std         3.694119                     88.377513                  4.100500   \n",
       "min         0.000000                      0.022000                  0.039000   \n",
       "25%         0.647000                    122.959750                  4.797000   \n",
       "50%         2.985500                    212.370500                  7.562000   \n",
       "75%         7.813000                    267.541000                 10.546250   \n",
       "max        10.000000                    359.989000                 33.057000   \n",
       "\n",
       "       air temperature at 2m (K)  surface air pressure (Pa)  \\\n",
       "count              105408.000000              105408.000000   \n",
       "mean                  287.464005               84606.872893   \n",
       "std                     9.776362                 468.523564   \n",
       "min                   264.279000               83157.784000   \n",
       "25%                   279.383000               84290.328000   \n",
       "50%                   287.852000               84634.568000   \n",
       "75%                   295.039000               84930.296000   \n",
       "max                   309.620000               86287.160000   \n",
       "\n",
       "       density at hub height (kg/m^3)  \n",
       "count                   105408.000000  \n",
       "mean                         1.011686  \n",
       "std                          0.033038  \n",
       "min                          0.939000  \n",
       "25%                          0.986000  \n",
       "50%                          1.008000  \n",
       "75%                          1.036000  \n",
       "max                          1.102000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ngboost import NGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/apple/Documents/ML_Project/ML - 2.1/Data/相近8个地点2012年数据/20739-2012.csv',\n",
    "                  skiprows=[0,1,2])\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:56:24.428799Z",
     "start_time": "2020-02-02T15:56:24.275698Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data.drop('power (MW)', axis=1)\n",
    "Y = data['power (MW)']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=520)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_Scaler = MinMaxScaler()\n",
    "X_train = X_Scaler.fit_transform(X_train)\n",
    "X_test = X_Scaler.transform(X_test)\n",
    "\n",
    "Y_Scaler = MinMaxScaler()\n",
    "Y_train = Y_Scaler.fit_transform(Y_train.values.reshape(-1,1)).reshape(len(Y_train),)\n",
    "Y_test = Y_Scaler.transform(Y_test.values.reshape(-1,1)).reshape(len(Y_test),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T16:06:08.569996Z",
     "start_time": "2020-02-02T16:06:08.554599Z"
    }
   },
   "outputs": [],
   "source": [
    "from ngboost import NGBRegressor\n",
    "def model_test(Base, X_train, X_test, Y_train, Y_test, \n",
    "               n_estimators=500, verbose_eval=100):\n",
    "    ngb = NGBRegressor(Base=Base, n_estimators=n_estimators,verbose_eval=verbose_eval)\n",
    "    print(ngb,'\\n')\n",
    "    ngb.fit(X_train, Y_train)\n",
    "\n",
    "    Y_preds = ngb.predict(X_test)\n",
    "    Y_dists = ngb.pred_dist(X_test) # return norm method: mean std\n",
    "\n",
    "    # test Mean Squared Error\n",
    "    test_MSE = mean_squared_error(Y_preds, Y_test)\n",
    "    print('\\nTest MSE', test_MSE)\n",
    "\n",
    "    # test Negative Log Likelihood\n",
    "    test_NLL = -Y_dists.logpdf(Y_test).mean()\n",
    "    print('Test NLL', test_NLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## default_linear_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T16:07:47.375846Z",
     "start_time": "2020-02-02T16:06:37.349460Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGBRegressor(Base=<function default_linear_learner at 0x1a1ea9b488>,\n",
      "             Dist=<class 'ngboost.distns.normal.Normal'>,\n",
      "             Score=<class 'ngboost.scores.MLE'>, learning_rate=0.01,\n",
      "             minibatch_frac=1.0, n_estimators=500, natural_gradient=True,\n",
      "             tol=0.0001, verbose=True, verbose_eval=100) \n",
      "\n",
      "[iter 0] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5075\n",
      "[iter 100] loss=-0.1296 val_loss=0.0000 scale=1.0000 norm=0.3964\n",
      "[iter 200] loss=-0.4713 val_loss=0.0000 scale=1.0000 norm=0.3808\n",
      "[iter 300] loss=-0.6817 val_loss=0.0000 scale=1.0000 norm=0.3446\n",
      "[iter 400] loss=-0.7670 val_loss=0.0000 scale=1.0000 norm=0.3483\n",
      "\n",
      "Test MSE 0.015955682176653666\n",
      "Test NLL -0.7861904540301601\n"
     ]
    }
   ],
   "source": [
    "from ngboost.learners import default_linear_learner\n",
    "model_test(Base=default_linear_learner,\n",
    "           X_train=X_train, X_test=X_test,\n",
    "           Y_train=Y_train, Y_test=Y_test,\n",
    "          n_estimators=500, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## default_tree_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T16:09:12.473232Z",
     "start_time": "2020-02-02T16:07:47.837817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGBRegressor(Base=<function default_linear_learner at 0x1a1ea9b488>,\n",
      "             Dist=<class 'ngboost.distns.normal.Normal'>,\n",
      "             Score=<class 'ngboost.scores.MLE'>, learning_rate=0.01,\n",
      "             minibatch_frac=1.0, n_estimators=500, natural_gradient=True,\n",
      "             tol=0.0001, verbose=True, verbose_eval=100) \n",
      "\n",
      "[iter 0] loss=0.4233 val_loss=0.0000 scale=1.0000 norm=0.5075\n",
      "[iter 100] loss=-0.1296 val_loss=0.0000 scale=1.0000 norm=0.3964\n",
      "[iter 200] loss=-0.4713 val_loss=0.0000 scale=1.0000 norm=0.3808\n",
      "[iter 300] loss=-0.6817 val_loss=0.0000 scale=1.0000 norm=0.3446\n",
      "[iter 400] loss=-0.7670 val_loss=0.0000 scale=1.0000 norm=0.3483\n",
      "\n",
      "Test MSE 0.015955682176653666\n",
      "Test NLL -0.7861904540301601\n"
     ]
    }
   ],
   "source": [
    "from ngboost.learners import default_tree_learner\n",
    "model_test(Base=default_tree_learner,\n",
    "           X_train=X_train, X_test=X_test,\n",
    "           Y_train=Y_train, Y_test=Y_test,\n",
    "          n_estimators=500, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T13:48:04.141127Z",
     "start_time": "2020-02-01T13:48:04.136843Z"
    }
   },
   "source": [
    "## esn_ridge_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T15:18:54.226312Z",
     "start_time": "2020-02-02T14:52:04.827849Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGBRegressor(Base=<function esn_ridge_learner at 0x1a1fceb7b8>,\n",
      "             Dist=<class 'ngboost.distns.normal.Normal'>,\n",
      "             Score=<class 'ngboost.scores.MLE'>, learning_rate=0.01,\n",
      "             minibatch_frac=1.0, n_estimators=500, natural_gradient=True,\n",
      "             tol=0.0001, verbose=True, verbose_eval=1)\n",
      "[iter 0] loss=2.7261 val_loss=0.0000 scale=1.0000 norm=3.3491\n",
      "[iter 1] loss=2.7167 val_loss=0.0000 scale=1.0000 norm=3.3216\n",
      "[iter 2] loss=2.7074 val_loss=0.0000 scale=1.0000 norm=3.2942\n",
      "[iter 3] loss=2.6984 val_loss=0.0000 scale=1.0000 norm=3.2673\n",
      "[iter 4] loss=2.6896 val_loss=0.0000 scale=1.0000 norm=3.2403\n",
      "[iter 5] loss=2.6810 val_loss=0.0000 scale=1.0000 norm=3.2139\n",
      "[iter 6] loss=2.6727 val_loss=0.0000 scale=1.0000 norm=3.1880\n",
      "[iter 7] loss=2.6644 val_loss=0.0000 scale=1.0000 norm=3.1619\n",
      "[iter 8] loss=2.6564 val_loss=0.0000 scale=1.0000 norm=3.1364\n",
      "[iter 9] loss=2.6485 val_loss=0.0000 scale=1.0000 norm=3.1110\n",
      "[iter 10] loss=2.6407 val_loss=0.0000 scale=1.0000 norm=3.0859\n",
      "[iter 11] loss=2.6332 val_loss=0.0000 scale=1.0000 norm=3.0614\n",
      "[iter 12] loss=2.6257 val_loss=0.0000 scale=1.0000 norm=3.0367\n",
      "[iter 13] loss=2.6183 val_loss=0.0000 scale=1.0000 norm=3.0124\n",
      "[iter 14] loss=2.6112 val_loss=0.0000 scale=1.0000 norm=2.9888\n",
      "[iter 15] loss=2.6040 val_loss=0.0000 scale=1.0000 norm=2.9648\n",
      "[iter 16] loss=2.5970 val_loss=0.0000 scale=1.0000 norm=2.9413\n",
      "[iter 17] loss=2.5901 val_loss=0.0000 scale=1.0000 norm=2.9181\n",
      "[iter 18] loss=2.5834 val_loss=0.0000 scale=1.0000 norm=2.8955\n",
      "[iter 19] loss=2.5767 val_loss=0.0000 scale=1.0000 norm=2.8728\n",
      "[iter 20] loss=2.5701 val_loss=0.0000 scale=1.0000 norm=2.8505\n",
      "[iter 21] loss=2.5635 val_loss=0.0000 scale=1.0000 norm=2.8281\n",
      "[iter 22] loss=2.5571 val_loss=0.0000 scale=1.0000 norm=2.8059\n",
      "[iter 23] loss=2.5507 val_loss=0.0000 scale=1.0000 norm=2.7842\n",
      "[iter 24] loss=2.5445 val_loss=0.0000 scale=1.0000 norm=2.7629\n",
      "[iter 25] loss=2.5383 val_loss=0.0000 scale=1.0000 norm=2.7416\n",
      "[iter 26] loss=2.5321 val_loss=0.0000 scale=1.0000 norm=2.7205\n",
      "[iter 27] loss=2.5261 val_loss=0.0000 scale=1.0000 norm=2.6997\n",
      "[iter 28] loss=2.5201 val_loss=0.0000 scale=1.0000 norm=2.6790\n",
      "[iter 29] loss=2.5141 val_loss=0.0000 scale=1.0000 norm=2.6585\n",
      "[iter 30] loss=2.5082 val_loss=0.0000 scale=1.0000 norm=2.6383\n",
      "[iter 31] loss=2.5024 val_loss=0.0000 scale=1.0000 norm=2.6182\n",
      "[iter 32] loss=2.4966 val_loss=0.0000 scale=1.0000 norm=2.5983\n",
      "[iter 33] loss=2.4909 val_loss=0.0000 scale=1.0000 norm=2.5786\n",
      "[iter 34] loss=2.4852 val_loss=0.0000 scale=1.0000 norm=2.5594\n",
      "[iter 35] loss=2.4797 val_loss=0.0000 scale=1.0000 norm=2.5405\n",
      "[iter 36] loss=2.4741 val_loss=0.0000 scale=1.0000 norm=2.5215\n",
      "[iter 37] loss=2.4687 val_loss=0.0000 scale=1.0000 norm=2.5029\n",
      "[iter 38] loss=2.4632 val_loss=0.0000 scale=1.0000 norm=2.4843\n",
      "[iter 39] loss=2.4578 val_loss=0.0000 scale=1.0000 norm=2.4662\n",
      "[iter 40] loss=2.4525 val_loss=0.0000 scale=1.0000 norm=2.4482\n",
      "[iter 41] loss=2.4472 val_loss=0.0000 scale=1.0000 norm=2.4302\n",
      "[iter 42] loss=2.4420 val_loss=0.0000 scale=1.0000 norm=2.4127\n",
      "[iter 43] loss=2.4367 val_loss=0.0000 scale=1.0000 norm=2.3949\n",
      "[iter 44] loss=2.4315 val_loss=0.0000 scale=1.0000 norm=2.3775\n",
      "[iter 45] loss=2.4263 val_loss=0.0000 scale=1.0000 norm=2.3602\n",
      "[iter 46] loss=2.4211 val_loss=0.0000 scale=1.0000 norm=2.3431\n",
      "[iter 47] loss=2.4160 val_loss=0.0000 scale=1.0000 norm=2.3261\n",
      "[iter 48] loss=2.4109 val_loss=0.0000 scale=1.0000 norm=2.3093\n",
      "[iter 49] loss=2.4058 val_loss=0.0000 scale=1.0000 norm=2.2926\n",
      "[iter 50] loss=2.4007 val_loss=0.0000 scale=1.0000 norm=2.2761\n",
      "[iter 51] loss=2.3957 val_loss=0.0000 scale=1.0000 norm=2.2599\n",
      "[iter 52] loss=2.3907 val_loss=0.0000 scale=1.0000 norm=2.2438\n",
      "[iter 53] loss=2.3858 val_loss=0.0000 scale=1.0000 norm=2.2279\n",
      "[iter 54] loss=2.3810 val_loss=0.0000 scale=1.0000 norm=2.2124\n",
      "[iter 55] loss=2.3761 val_loss=0.0000 scale=1.0000 norm=2.1968\n",
      "[iter 56] loss=2.3712 val_loss=0.0000 scale=1.0000 norm=2.1816\n",
      "[iter 57] loss=2.3664 val_loss=0.0000 scale=1.0000 norm=2.1664\n",
      "[iter 58] loss=2.3616 val_loss=0.0000 scale=1.0000 norm=2.1512\n",
      "[iter 59] loss=2.3568 val_loss=0.0000 scale=1.0000 norm=2.1363\n",
      "[iter 60] loss=2.3521 val_loss=0.0000 scale=1.0000 norm=2.1217\n",
      "[iter 61] loss=2.3474 val_loss=0.0000 scale=1.0000 norm=2.1074\n",
      "[iter 62] loss=2.3427 val_loss=0.0000 scale=1.0000 norm=2.0930\n",
      "[iter 63] loss=2.3380 val_loss=0.0000 scale=1.0000 norm=2.0785\n",
      "[iter 64] loss=2.3333 val_loss=0.0000 scale=1.0000 norm=2.0643\n",
      "[iter 65] loss=2.3286 val_loss=0.0000 scale=1.0000 norm=2.0504\n",
      "[iter 66] loss=2.3239 val_loss=0.0000 scale=1.0000 norm=2.0364\n",
      "[iter 67] loss=2.3193 val_loss=0.0000 scale=1.0000 norm=2.0228\n",
      "[iter 68] loss=2.3148 val_loss=0.0000 scale=1.0000 norm=2.0094\n",
      "[iter 69] loss=2.3102 val_loss=0.0000 scale=1.0000 norm=1.9960\n",
      "[iter 70] loss=2.3056 val_loss=0.0000 scale=1.0000 norm=1.9828\n",
      "[iter 71] loss=2.3011 val_loss=0.0000 scale=1.0000 norm=1.9699\n",
      "[iter 72] loss=2.2966 val_loss=0.0000 scale=1.0000 norm=1.9570\n",
      "[iter 73] loss=2.2921 val_loss=0.0000 scale=1.0000 norm=1.9443\n",
      "[iter 74] loss=2.2877 val_loss=0.0000 scale=1.0000 norm=1.9318\n",
      "[iter 75] loss=2.2832 val_loss=0.0000 scale=1.0000 norm=1.9195\n",
      "[iter 76] loss=2.2788 val_loss=0.0000 scale=1.0000 norm=1.9074\n",
      "[iter 77] loss=2.2744 val_loss=0.0000 scale=1.0000 norm=1.8952\n",
      "[iter 78] loss=2.2700 val_loss=0.0000 scale=1.0000 norm=1.8834\n",
      "[iter 79] loss=2.2657 val_loss=0.0000 scale=1.0000 norm=1.8718\n",
      "[iter 80] loss=2.2613 val_loss=0.0000 scale=1.0000 norm=1.8601\n",
      "[iter 81] loss=2.2570 val_loss=0.0000 scale=1.0000 norm=1.8487\n",
      "[iter 82] loss=2.2527 val_loss=0.0000 scale=1.0000 norm=1.8374\n",
      "[iter 83] loss=2.2484 val_loss=0.0000 scale=1.0000 norm=1.8262\n",
      "[iter 84] loss=2.2441 val_loss=0.0000 scale=1.0000 norm=1.8151\n",
      "[iter 85] loss=2.2398 val_loss=0.0000 scale=1.0000 norm=1.8043\n",
      "[iter 86] loss=2.2356 val_loss=0.0000 scale=1.0000 norm=1.7936\n",
      "[iter 87] loss=2.2313 val_loss=0.0000 scale=1.0000 norm=1.7829\n",
      "[iter 88] loss=2.2271 val_loss=0.0000 scale=1.0000 norm=1.7726\n",
      "[iter 89] loss=2.2229 val_loss=0.0000 scale=1.0000 norm=1.7621\n",
      "[iter 90] loss=2.2186 val_loss=0.0000 scale=1.0000 norm=1.7519\n",
      "[iter 91] loss=2.2144 val_loss=0.0000 scale=1.0000 norm=1.7417\n",
      "[iter 92] loss=2.2102 val_loss=0.0000 scale=1.0000 norm=1.7316\n",
      "[iter 93] loss=2.2060 val_loss=0.0000 scale=1.0000 norm=1.7218\n",
      "[iter 94] loss=2.2019 val_loss=0.0000 scale=1.0000 norm=1.7122\n",
      "[iter 95] loss=2.1978 val_loss=0.0000 scale=1.0000 norm=1.7027\n",
      "[iter 96] loss=2.1937 val_loss=0.0000 scale=1.0000 norm=1.6935\n",
      "[iter 97] loss=2.1896 val_loss=0.0000 scale=1.0000 norm=1.6844\n",
      "[iter 98] loss=2.1855 val_loss=0.0000 scale=1.0000 norm=1.6752\n",
      "[iter 99] loss=2.1814 val_loss=0.0000 scale=1.0000 norm=1.6661\n",
      "[iter 100] loss=2.1774 val_loss=0.0000 scale=1.0000 norm=1.6574\n",
      "[iter 101] loss=2.1734 val_loss=0.0000 scale=1.0000 norm=1.6487\n",
      "[iter 102] loss=2.1693 val_loss=0.0000 scale=1.0000 norm=1.6400\n",
      "[iter 103] loss=2.1653 val_loss=0.0000 scale=1.0000 norm=1.6315\n",
      "[iter 104] loss=2.1613 val_loss=0.0000 scale=1.0000 norm=1.6231\n",
      "[iter 105] loss=2.1573 val_loss=0.0000 scale=1.0000 norm=1.6148\n",
      "[iter 106] loss=2.1532 val_loss=0.0000 scale=1.0000 norm=1.6065\n",
      "[iter 107] loss=2.1493 val_loss=0.0000 scale=1.0000 norm=1.5985\n",
      "[iter 108] loss=2.1453 val_loss=0.0000 scale=1.0000 norm=1.5905\n",
      "[iter 109] loss=2.1413 val_loss=0.0000 scale=1.0000 norm=1.5826\n",
      "[iter 110] loss=2.1373 val_loss=0.0000 scale=1.0000 norm=1.5749\n",
      "[iter 111] loss=2.1334 val_loss=0.0000 scale=1.0000 norm=1.5674\n",
      "[iter 112] loss=2.1295 val_loss=0.0000 scale=1.0000 norm=1.5599\n",
      "[iter 113] loss=2.1256 val_loss=0.0000 scale=1.0000 norm=1.5527\n",
      "[iter 114] loss=2.1217 val_loss=0.0000 scale=1.0000 norm=1.5452\n",
      "[iter 115] loss=2.1178 val_loss=0.0000 scale=1.0000 norm=1.5382\n",
      "[iter 116] loss=2.1140 val_loss=0.0000 scale=1.0000 norm=1.5311\n",
      "[iter 117] loss=2.1101 val_loss=0.0000 scale=1.0000 norm=1.5241\n",
      "[iter 118] loss=2.1062 val_loss=0.0000 scale=1.0000 norm=1.5173\n",
      "[iter 119] loss=2.1024 val_loss=0.0000 scale=1.0000 norm=1.5107\n",
      "[iter 120] loss=2.0986 val_loss=0.0000 scale=1.0000 norm=1.5041\n",
      "[iter 121] loss=2.0948 val_loss=0.0000 scale=1.0000 norm=1.4974\n",
      "[iter 122] loss=2.0910 val_loss=0.0000 scale=1.0000 norm=1.4911\n",
      "[iter 123] loss=2.0872 val_loss=0.0000 scale=1.0000 norm=1.4848\n",
      "[iter 124] loss=2.0834 val_loss=0.0000 scale=1.0000 norm=1.4786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 125] loss=2.0796 val_loss=0.0000 scale=1.0000 norm=1.4725\n",
      "[iter 126] loss=2.0759 val_loss=0.0000 scale=1.0000 norm=1.4665\n",
      "[iter 127] loss=2.0721 val_loss=0.0000 scale=1.0000 norm=1.4605\n",
      "[iter 128] loss=2.0684 val_loss=0.0000 scale=1.0000 norm=1.4546\n",
      "[iter 129] loss=2.0647 val_loss=0.0000 scale=1.0000 norm=1.4489\n",
      "[iter 130] loss=2.0610 val_loss=0.0000 scale=1.0000 norm=1.4433\n",
      "[iter 131] loss=2.0573 val_loss=0.0000 scale=1.0000 norm=1.4379\n",
      "[iter 132] loss=2.0537 val_loss=0.0000 scale=1.0000 norm=1.4327\n",
      "[iter 133] loss=2.0501 val_loss=0.0000 scale=1.0000 norm=1.4274\n",
      "[iter 134] loss=2.0464 val_loss=0.0000 scale=1.0000 norm=1.4219\n",
      "[iter 135] loss=2.0428 val_loss=0.0000 scale=1.0000 norm=1.4166\n",
      "[iter 136] loss=2.0391 val_loss=0.0000 scale=1.0000 norm=1.4114\n",
      "[iter 137] loss=2.0355 val_loss=0.0000 scale=1.0000 norm=1.4063\n",
      "[iter 138] loss=2.0319 val_loss=0.0000 scale=1.0000 norm=1.4013\n",
      "[iter 139] loss=2.0283 val_loss=0.0000 scale=1.0000 norm=1.3964\n",
      "[iter 140] loss=2.0247 val_loss=0.0000 scale=1.0000 norm=1.3916\n",
      "[iter 141] loss=2.0211 val_loss=0.0000 scale=1.0000 norm=1.3867\n",
      "[iter 142] loss=2.0175 val_loss=0.0000 scale=1.0000 norm=1.3820\n",
      "[iter 143] loss=2.0139 val_loss=0.0000 scale=1.0000 norm=1.3773\n",
      "[iter 144] loss=2.0104 val_loss=0.0000 scale=1.0000 norm=1.3728\n",
      "[iter 145] loss=2.0069 val_loss=0.0000 scale=1.0000 norm=1.3684\n",
      "[iter 146] loss=2.0034 val_loss=0.0000 scale=1.0000 norm=1.3641\n",
      "[iter 147] loss=1.9999 val_loss=0.0000 scale=1.0000 norm=1.3598\n",
      "[iter 148] loss=1.9964 val_loss=0.0000 scale=1.0000 norm=1.3555\n",
      "[iter 149] loss=1.9929 val_loss=0.0000 scale=1.0000 norm=1.3513\n",
      "[iter 150] loss=1.9895 val_loss=0.0000 scale=1.0000 norm=1.3473\n",
      "[iter 151] loss=1.9860 val_loss=0.0000 scale=1.0000 norm=1.3432\n",
      "[iter 152] loss=1.9826 val_loss=0.0000 scale=1.0000 norm=1.3392\n",
      "[iter 153] loss=1.9792 val_loss=0.0000 scale=1.0000 norm=1.3353\n",
      "[iter 154] loss=1.9757 val_loss=0.0000 scale=1.0000 norm=1.3314\n",
      "[iter 155] loss=1.9723 val_loss=0.0000 scale=1.0000 norm=1.3276\n",
      "[iter 156] loss=1.9689 val_loss=0.0000 scale=1.0000 norm=1.3238\n",
      "[iter 157] loss=1.9655 val_loss=0.0000 scale=1.0000 norm=1.3200\n",
      "[iter 158] loss=1.9621 val_loss=0.0000 scale=1.0000 norm=1.3163\n",
      "[iter 159] loss=1.9588 val_loss=0.0000 scale=1.0000 norm=1.3128\n",
      "[iter 160] loss=1.9554 val_loss=0.0000 scale=1.0000 norm=1.3092\n",
      "[iter 161] loss=1.9521 val_loss=0.0000 scale=1.0000 norm=1.3057\n",
      "[iter 162] loss=1.9488 val_loss=0.0000 scale=1.0000 norm=1.3023\n",
      "[iter 163] loss=1.9455 val_loss=0.0000 scale=1.0000 norm=1.2990\n",
      "[iter 164] loss=1.9422 val_loss=0.0000 scale=1.0000 norm=1.2957\n",
      "[iter 165] loss=1.9389 val_loss=0.0000 scale=1.0000 norm=1.2924\n",
      "[iter 166] loss=1.9356 val_loss=0.0000 scale=1.0000 norm=1.2891\n",
      "[iter 167] loss=1.9323 val_loss=0.0000 scale=1.0000 norm=1.2859\n",
      "[iter 168] loss=1.9291 val_loss=0.0000 scale=1.0000 norm=1.2828\n",
      "[iter 169] loss=1.9258 val_loss=0.0000 scale=1.0000 norm=1.2797\n",
      "[iter 170] loss=1.9226 val_loss=0.0000 scale=1.0000 norm=1.2766\n",
      "[iter 171] loss=1.9194 val_loss=0.0000 scale=1.0000 norm=1.2737\n",
      "[iter 172] loss=1.9162 val_loss=0.0000 scale=1.0000 norm=1.2708\n",
      "[iter 173] loss=1.9130 val_loss=0.0000 scale=1.0000 norm=1.2679\n",
      "[iter 174] loss=1.9098 val_loss=0.0000 scale=1.0000 norm=1.2649\n",
      "[iter 175] loss=1.9067 val_loss=0.0000 scale=1.0000 norm=1.2622\n",
      "[iter 176] loss=1.9035 val_loss=0.0000 scale=1.0000 norm=1.2594\n",
      "[iter 177] loss=1.9004 val_loss=0.0000 scale=1.0000 norm=1.2566\n",
      "[iter 178] loss=1.8973 val_loss=0.0000 scale=1.0000 norm=1.2540\n",
      "[iter 179] loss=1.8942 val_loss=0.0000 scale=1.0000 norm=1.2513\n",
      "[iter 180] loss=1.8911 val_loss=0.0000 scale=1.0000 norm=1.2487\n",
      "[iter 181] loss=1.8880 val_loss=0.0000 scale=1.0000 norm=1.2461\n",
      "[iter 182] loss=1.8849 val_loss=0.0000 scale=1.0000 norm=1.2436\n",
      "[iter 183] loss=1.8818 val_loss=0.0000 scale=1.0000 norm=1.2410\n",
      "[iter 184] loss=1.8787 val_loss=0.0000 scale=1.0000 norm=1.2385\n",
      "[iter 185] loss=1.8757 val_loss=0.0000 scale=1.0000 norm=1.2361\n",
      "[iter 186] loss=1.8727 val_loss=0.0000 scale=1.0000 norm=1.2337\n",
      "[iter 187] loss=1.8697 val_loss=0.0000 scale=1.0000 norm=1.2314\n",
      "[iter 188] loss=1.8667 val_loss=0.0000 scale=1.0000 norm=1.2290\n",
      "[iter 189] loss=1.8637 val_loss=0.0000 scale=1.0000 norm=1.2267\n",
      "[iter 190] loss=1.8607 val_loss=0.0000 scale=1.0000 norm=1.2245\n",
      "[iter 191] loss=1.8577 val_loss=0.0000 scale=1.0000 norm=1.2222\n",
      "[iter 192] loss=1.8548 val_loss=0.0000 scale=1.0000 norm=1.2201\n",
      "[iter 193] loss=1.8519 val_loss=0.0000 scale=1.0000 norm=1.2179\n",
      "[iter 194] loss=1.8489 val_loss=0.0000 scale=1.0000 norm=1.2158\n",
      "[iter 195] loss=1.8460 val_loss=0.0000 scale=1.0000 norm=1.2136\n",
      "[iter 196] loss=1.8431 val_loss=0.0000 scale=1.0000 norm=1.2115\n",
      "[iter 197] loss=1.8402 val_loss=0.0000 scale=1.0000 norm=1.2095\n",
      "[iter 198] loss=1.8374 val_loss=0.0000 scale=1.0000 norm=1.2075\n",
      "[iter 199] loss=1.8345 val_loss=0.0000 scale=1.0000 norm=1.2056\n",
      "[iter 200] loss=1.8317 val_loss=0.0000 scale=1.0000 norm=1.2036\n",
      "[iter 201] loss=1.8289 val_loss=0.0000 scale=1.0000 norm=1.2017\n",
      "[iter 202] loss=1.8260 val_loss=0.0000 scale=1.0000 norm=1.1997\n",
      "[iter 203] loss=1.8232 val_loss=0.0000 scale=1.0000 norm=1.1979\n",
      "[iter 204] loss=1.8204 val_loss=0.0000 scale=1.0000 norm=1.1960\n",
      "[iter 205] loss=1.8176 val_loss=0.0000 scale=1.0000 norm=1.1941\n",
      "[iter 206] loss=1.8149 val_loss=0.0000 scale=1.0000 norm=1.1924\n",
      "[iter 207] loss=1.8121 val_loss=0.0000 scale=1.0000 norm=1.1906\n",
      "[iter 208] loss=1.8094 val_loss=0.0000 scale=1.0000 norm=1.1888\n",
      "[iter 209] loss=1.8067 val_loss=0.0000 scale=1.0000 norm=1.1871\n",
      "[iter 210] loss=1.8040 val_loss=0.0000 scale=1.0000 norm=1.1854\n",
      "[iter 211] loss=1.8013 val_loss=0.0000 scale=1.0000 norm=1.1837\n",
      "[iter 212] loss=1.7985 val_loss=0.0000 scale=1.0000 norm=1.1820\n",
      "[iter 213] loss=1.7959 val_loss=0.0000 scale=1.0000 norm=1.1803\n",
      "[iter 214] loss=1.7932 val_loss=0.0000 scale=1.0000 norm=1.1787\n",
      "[iter 215] loss=1.7906 val_loss=0.0000 scale=1.0000 norm=1.1771\n",
      "[iter 216] loss=1.7879 val_loss=0.0000 scale=1.0000 norm=1.1756\n",
      "[iter 217] loss=1.7853 val_loss=0.0000 scale=1.0000 norm=1.1740\n",
      "[iter 218] loss=1.7827 val_loss=0.0000 scale=1.0000 norm=1.1725\n",
      "[iter 219] loss=1.7801 val_loss=0.0000 scale=1.0000 norm=1.1710\n",
      "[iter 220] loss=1.7775 val_loss=0.0000 scale=1.0000 norm=1.1695\n",
      "[iter 221] loss=1.7750 val_loss=0.0000 scale=1.0000 norm=1.1680\n",
      "[iter 222] loss=1.7724 val_loss=0.0000 scale=1.0000 norm=1.1666\n",
      "[iter 223] loss=1.7699 val_loss=0.0000 scale=1.0000 norm=1.1652\n",
      "[iter 224] loss=1.7674 val_loss=0.0000 scale=1.0000 norm=1.1637\n",
      "[iter 225] loss=1.7649 val_loss=0.0000 scale=1.0000 norm=1.1623\n",
      "[iter 226] loss=1.7624 val_loss=0.0000 scale=1.0000 norm=1.1610\n",
      "[iter 227] loss=1.7599 val_loss=0.0000 scale=1.0000 norm=1.1596\n",
      "[iter 228] loss=1.7575 val_loss=0.0000 scale=1.0000 norm=1.1583\n",
      "[iter 229] loss=1.7550 val_loss=0.0000 scale=1.0000 norm=1.1570\n",
      "[iter 230] loss=1.7526 val_loss=0.0000 scale=1.0000 norm=1.1557\n",
      "[iter 231] loss=1.7501 val_loss=0.0000 scale=1.0000 norm=1.1543\n",
      "[iter 232] loss=1.7477 val_loss=0.0000 scale=1.0000 norm=1.1530\n",
      "[iter 233] loss=1.7453 val_loss=0.0000 scale=1.0000 norm=1.1517\n",
      "[iter 234] loss=1.7429 val_loss=0.0000 scale=1.0000 norm=1.1505\n",
      "[iter 235] loss=1.7405 val_loss=0.0000 scale=1.0000 norm=1.1493\n",
      "[iter 236] loss=1.7382 val_loss=0.0000 scale=1.0000 norm=1.1481\n",
      "[iter 237] loss=1.7358 val_loss=0.0000 scale=1.0000 norm=1.1469\n",
      "[iter 238] loss=1.7335 val_loss=0.0000 scale=1.0000 norm=1.1457\n",
      "[iter 239] loss=1.7312 val_loss=0.0000 scale=1.0000 norm=1.1446\n",
      "[iter 240] loss=1.7289 val_loss=0.0000 scale=1.0000 norm=1.1434\n",
      "[iter 241] loss=1.7266 val_loss=0.0000 scale=1.0000 norm=1.1423\n",
      "[iter 242] loss=1.7243 val_loss=0.0000 scale=1.0000 norm=1.1411\n",
      "[iter 243] loss=1.7220 val_loss=0.0000 scale=1.0000 norm=1.1400\n",
      "[iter 244] loss=1.7197 val_loss=0.0000 scale=1.0000 norm=1.1389\n",
      "[iter 245] loss=1.7175 val_loss=0.0000 scale=1.0000 norm=1.1379\n",
      "[iter 246] loss=1.7153 val_loss=0.0000 scale=1.0000 norm=1.1368\n",
      "[iter 247] loss=1.7131 val_loss=0.0000 scale=1.0000 norm=1.1358\n",
      "[iter 248] loss=1.7109 val_loss=0.0000 scale=1.0000 norm=1.1347\n",
      "[iter 249] loss=1.7087 val_loss=0.0000 scale=1.0000 norm=1.1337\n",
      "[iter 250] loss=1.7066 val_loss=0.0000 scale=1.0000 norm=1.1327\n",
      "[iter 251] loss=1.7044 val_loss=0.0000 scale=1.0000 norm=1.1317\n",
      "[iter 252] loss=1.7023 val_loss=0.0000 scale=1.0000 norm=1.1307\n",
      "[iter 253] loss=1.7002 val_loss=0.0000 scale=1.0000 norm=1.1297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 254] loss=1.6981 val_loss=0.0000 scale=1.0000 norm=1.1288\n",
      "[iter 255] loss=1.6960 val_loss=0.0000 scale=1.0000 norm=1.1279\n",
      "[iter 256] loss=1.6939 val_loss=0.0000 scale=1.0000 norm=1.1269\n",
      "[iter 257] loss=1.6918 val_loss=0.0000 scale=1.0000 norm=1.1260\n",
      "[iter 258] loss=1.6898 val_loss=0.0000 scale=1.0000 norm=1.1251\n",
      "[iter 259] loss=1.6878 val_loss=0.0000 scale=1.0000 norm=1.1242\n",
      "[iter 260] loss=1.6858 val_loss=0.0000 scale=1.0000 norm=1.1234\n",
      "[iter 261] loss=1.6838 val_loss=0.0000 scale=1.0000 norm=1.1225\n",
      "[iter 262] loss=1.6818 val_loss=0.0000 scale=1.0000 norm=1.1217\n",
      "[iter 263] loss=1.6798 val_loss=0.0000 scale=1.0000 norm=1.1208\n",
      "[iter 264] loss=1.6778 val_loss=0.0000 scale=1.0000 norm=1.1200\n",
      "[iter 265] loss=1.6759 val_loss=0.0000 scale=1.0000 norm=1.1191\n",
      "[iter 266] loss=1.6739 val_loss=0.0000 scale=1.0000 norm=1.1184\n",
      "[iter 267] loss=1.6720 val_loss=0.0000 scale=1.0000 norm=1.1175\n",
      "[iter 268] loss=1.6701 val_loss=0.0000 scale=1.0000 norm=1.1167\n",
      "[iter 269] loss=1.6682 val_loss=0.0000 scale=1.0000 norm=1.1160\n",
      "[iter 270] loss=1.6663 val_loss=0.0000 scale=1.0000 norm=1.1152\n",
      "[iter 271] loss=1.6645 val_loss=0.0000 scale=1.0000 norm=1.1144\n",
      "[iter 272] loss=1.6626 val_loss=0.0000 scale=1.0000 norm=1.1137\n",
      "[iter 273] loss=1.6607 val_loss=0.0000 scale=1.0000 norm=1.1129\n",
      "[iter 274] loss=1.6589 val_loss=0.0000 scale=1.0000 norm=1.1121\n",
      "[iter 275] loss=1.6571 val_loss=0.0000 scale=1.0000 norm=1.1113\n",
      "[iter 276] loss=1.6553 val_loss=0.0000 scale=1.0000 norm=1.1107\n",
      "[iter 277] loss=1.6535 val_loss=0.0000 scale=1.0000 norm=1.1100\n",
      "[iter 278] loss=1.6517 val_loss=0.0000 scale=1.0000 norm=1.1092\n",
      "[iter 279] loss=1.6499 val_loss=0.0000 scale=1.0000 norm=1.1085\n",
      "[iter 280] loss=1.6482 val_loss=0.0000 scale=1.0000 norm=1.1079\n",
      "[iter 281] loss=1.6464 val_loss=0.0000 scale=1.0000 norm=1.1072\n",
      "[iter 282] loss=1.6447 val_loss=0.0000 scale=1.0000 norm=1.1066\n",
      "[iter 283] loss=1.6430 val_loss=0.0000 scale=1.0000 norm=1.1059\n",
      "[iter 284] loss=1.6413 val_loss=0.0000 scale=1.0000 norm=1.1052\n",
      "[iter 285] loss=1.6396 val_loss=0.0000 scale=1.0000 norm=1.1046\n",
      "[iter 286] loss=1.6379 val_loss=0.0000 scale=1.0000 norm=1.1039\n",
      "[iter 287] loss=1.6363 val_loss=0.0000 scale=1.0000 norm=1.1033\n",
      "[iter 288] loss=1.6346 val_loss=0.0000 scale=1.0000 norm=1.1027\n",
      "[iter 289] loss=1.6330 val_loss=0.0000 scale=1.0000 norm=1.1021\n",
      "[iter 290] loss=1.6313 val_loss=0.0000 scale=1.0000 norm=1.1015\n",
      "[iter 291] loss=1.6297 val_loss=0.0000 scale=1.0000 norm=1.1009\n",
      "[iter 292] loss=1.6281 val_loss=0.0000 scale=1.0000 norm=1.1003\n",
      "[iter 293] loss=1.6265 val_loss=0.0000 scale=1.0000 norm=1.0996\n",
      "[iter 294] loss=1.6250 val_loss=0.0000 scale=1.0000 norm=1.0991\n",
      "[iter 295] loss=1.6234 val_loss=0.0000 scale=1.0000 norm=1.0985\n",
      "[iter 296] loss=1.6219 val_loss=0.0000 scale=1.0000 norm=1.0979\n",
      "[iter 297] loss=1.6203 val_loss=0.0000 scale=1.0000 norm=1.0974\n",
      "[iter 298] loss=1.6188 val_loss=0.0000 scale=1.0000 norm=1.0969\n",
      "[iter 299] loss=1.6173 val_loss=0.0000 scale=1.0000 norm=1.0963\n",
      "[iter 300] loss=1.6158 val_loss=0.0000 scale=1.0000 norm=1.0958\n",
      "[iter 301] loss=1.6144 val_loss=0.0000 scale=1.0000 norm=1.0953\n",
      "[iter 302] loss=1.6129 val_loss=0.0000 scale=1.0000 norm=1.0948\n",
      "[iter 303] loss=1.6114 val_loss=0.0000 scale=1.0000 norm=1.0943\n",
      "[iter 304] loss=1.6100 val_loss=0.0000 scale=1.0000 norm=1.0938\n",
      "[iter 305] loss=1.6086 val_loss=0.0000 scale=1.0000 norm=1.0933\n",
      "[iter 306] loss=1.6071 val_loss=0.0000 scale=1.0000 norm=1.0928\n",
      "[iter 307] loss=1.6057 val_loss=0.0000 scale=1.0000 norm=1.0924\n",
      "[iter 308] loss=1.6043 val_loss=0.0000 scale=1.0000 norm=1.0919\n",
      "[iter 309] loss=1.6030 val_loss=0.0000 scale=1.0000 norm=1.0915\n",
      "[iter 310] loss=1.6016 val_loss=0.0000 scale=1.0000 norm=1.0911\n",
      "[iter 311] loss=1.6002 val_loss=0.0000 scale=1.0000 norm=1.0906\n",
      "[iter 312] loss=1.5989 val_loss=0.0000 scale=1.0000 norm=1.0902\n",
      "[iter 313] loss=1.5976 val_loss=0.0000 scale=1.0000 norm=1.0898\n",
      "[iter 314] loss=1.5962 val_loss=0.0000 scale=1.0000 norm=1.0893\n",
      "[iter 315] loss=1.5949 val_loss=0.0000 scale=1.0000 norm=1.0889\n",
      "[iter 316] loss=1.5936 val_loss=0.0000 scale=1.0000 norm=1.0885\n",
      "[iter 317] loss=1.5923 val_loss=0.0000 scale=1.0000 norm=1.0881\n",
      "[iter 318] loss=1.5910 val_loss=0.0000 scale=1.0000 norm=1.0876\n",
      "[iter 319] loss=1.5898 val_loss=0.0000 scale=1.0000 norm=1.0872\n",
      "[iter 320] loss=1.5885 val_loss=0.0000 scale=1.0000 norm=1.0869\n",
      "[iter 321] loss=1.5873 val_loss=0.0000 scale=1.0000 norm=1.0865\n",
      "[iter 322] loss=1.5861 val_loss=0.0000 scale=1.0000 norm=1.0861\n",
      "[iter 323] loss=1.5849 val_loss=0.0000 scale=1.0000 norm=1.0858\n",
      "[iter 324] loss=1.5837 val_loss=0.0000 scale=1.0000 norm=1.0854\n",
      "[iter 325] loss=1.5824 val_loss=0.0000 scale=1.0000 norm=1.0850\n",
      "[iter 326] loss=1.5813 val_loss=0.0000 scale=1.0000 norm=1.0846\n",
      "[iter 327] loss=1.5801 val_loss=0.0000 scale=1.0000 norm=1.0843\n",
      "[iter 328] loss=1.5789 val_loss=0.0000 scale=1.0000 norm=1.0839\n",
      "[iter 329] loss=1.5778 val_loss=0.0000 scale=1.0000 norm=1.0836\n",
      "[iter 330] loss=1.5767 val_loss=0.0000 scale=1.0000 norm=1.0833\n",
      "[iter 331] loss=1.5755 val_loss=0.0000 scale=1.0000 norm=1.0830\n",
      "[iter 332] loss=1.5744 val_loss=0.0000 scale=1.0000 norm=1.0826\n",
      "[iter 333] loss=1.5733 val_loss=0.0000 scale=1.0000 norm=1.0823\n",
      "[iter 334] loss=1.5722 val_loss=0.0000 scale=1.0000 norm=1.0819\n",
      "[iter 335] loss=1.5711 val_loss=0.0000 scale=1.0000 norm=1.0817\n",
      "[iter 336] loss=1.5700 val_loss=0.0000 scale=1.0000 norm=1.0814\n",
      "[iter 337] loss=1.5690 val_loss=0.0000 scale=1.0000 norm=1.0811\n",
      "[iter 338] loss=1.5679 val_loss=0.0000 scale=1.0000 norm=1.0808\n",
      "[iter 339] loss=1.5668 val_loss=0.0000 scale=1.0000 norm=1.0805\n",
      "[iter 340] loss=1.5658 val_loss=0.0000 scale=1.0000 norm=1.0802\n",
      "[iter 341] loss=1.5648 val_loss=0.0000 scale=1.0000 norm=1.0799\n",
      "[iter 342] loss=1.5637 val_loss=0.0000 scale=1.0000 norm=1.0796\n",
      "[iter 343] loss=1.5627 val_loss=0.0000 scale=1.0000 norm=1.0794\n",
      "[iter 344] loss=1.5617 val_loss=0.0000 scale=1.0000 norm=1.0791\n",
      "[iter 345] loss=1.5607 val_loss=0.0000 scale=1.0000 norm=1.0788\n",
      "[iter 346] loss=1.5598 val_loss=0.0000 scale=1.0000 norm=1.0786\n",
      "[iter 347] loss=1.5588 val_loss=0.0000 scale=1.0000 norm=1.0784\n",
      "[iter 348] loss=1.5579 val_loss=0.0000 scale=1.0000 norm=1.0781\n",
      "[iter 349] loss=1.5569 val_loss=0.0000 scale=1.0000 norm=1.0779\n",
      "[iter 350] loss=1.5560 val_loss=0.0000 scale=1.0000 norm=1.0776\n",
      "[iter 351] loss=1.5551 val_loss=0.0000 scale=1.0000 norm=1.0774\n",
      "[iter 352] loss=1.5541 val_loss=0.0000 scale=1.0000 norm=1.0772\n",
      "[iter 353] loss=1.5532 val_loss=0.0000 scale=1.0000 norm=1.0769\n",
      "[iter 354] loss=1.5523 val_loss=0.0000 scale=1.0000 norm=1.0767\n",
      "[iter 355] loss=1.5514 val_loss=0.0000 scale=1.0000 norm=1.0765\n",
      "[iter 356] loss=1.5506 val_loss=0.0000 scale=1.0000 norm=1.0763\n",
      "[iter 357] loss=1.5497 val_loss=0.0000 scale=1.0000 norm=1.0761\n",
      "[iter 358] loss=1.5488 val_loss=0.0000 scale=1.0000 norm=1.0758\n",
      "[iter 359] loss=1.5479 val_loss=0.0000 scale=1.0000 norm=1.0756\n",
      "[iter 360] loss=1.5471 val_loss=0.0000 scale=1.0000 norm=1.0754\n",
      "[iter 361] loss=1.5463 val_loss=0.0000 scale=1.0000 norm=1.0752\n",
      "[iter 362] loss=1.5454 val_loss=0.0000 scale=1.0000 norm=1.0750\n",
      "[iter 363] loss=1.5446 val_loss=0.0000 scale=1.0000 norm=1.0748\n",
      "[iter 364] loss=1.5438 val_loss=0.0000 scale=1.0000 norm=1.0746\n",
      "[iter 365] loss=1.5430 val_loss=0.0000 scale=1.0000 norm=1.0744\n",
      "[iter 366] loss=1.5421 val_loss=0.0000 scale=1.0000 norm=1.0742\n",
      "[iter 367] loss=1.5413 val_loss=0.0000 scale=1.0000 norm=1.0740\n",
      "[iter 368] loss=1.5406 val_loss=0.0000 scale=1.0000 norm=1.0739\n",
      "[iter 369] loss=1.5398 val_loss=0.0000 scale=1.0000 norm=1.0737\n",
      "[iter 370] loss=1.5390 val_loss=0.0000 scale=1.0000 norm=1.0735\n",
      "[iter 371] loss=1.5383 val_loss=0.0000 scale=1.0000 norm=1.0734\n",
      "[iter 372] loss=1.5375 val_loss=0.0000 scale=1.0000 norm=1.0732\n",
      "[iter 373] loss=1.5368 val_loss=0.0000 scale=1.0000 norm=1.0731\n",
      "[iter 374] loss=1.5360 val_loss=0.0000 scale=1.0000 norm=1.0729\n",
      "[iter 375] loss=1.5353 val_loss=0.0000 scale=1.0000 norm=1.0727\n",
      "[iter 376] loss=1.5346 val_loss=0.0000 scale=1.0000 norm=1.0726\n",
      "[iter 377] loss=1.5339 val_loss=0.0000 scale=1.0000 norm=1.0725\n",
      "[iter 378] loss=1.5332 val_loss=0.0000 scale=1.0000 norm=1.0723\n",
      "[iter 379] loss=1.5325 val_loss=0.0000 scale=1.0000 norm=1.0722\n",
      "[iter 380] loss=1.5318 val_loss=0.0000 scale=1.0000 norm=1.0721\n",
      "[iter 381] loss=1.5311 val_loss=0.0000 scale=1.0000 norm=1.0720\n",
      "[iter 382] loss=1.5305 val_loss=0.0000 scale=1.0000 norm=1.0718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 383] loss=1.5298 val_loss=0.0000 scale=1.0000 norm=1.0717\n",
      "[iter 384] loss=1.5291 val_loss=0.0000 scale=1.0000 norm=1.0716\n",
      "[iter 385] loss=1.5284 val_loss=0.0000 scale=1.0000 norm=1.0714\n",
      "[iter 386] loss=1.5278 val_loss=0.0000 scale=1.0000 norm=1.0713\n",
      "[iter 387] loss=1.5271 val_loss=0.0000 scale=1.0000 norm=1.0712\n",
      "[iter 388] loss=1.5264 val_loss=0.0000 scale=1.0000 norm=1.0710\n",
      "[iter 389] loss=1.5258 val_loss=0.0000 scale=1.0000 norm=1.0709\n",
      "[iter 390] loss=1.5252 val_loss=0.0000 scale=1.0000 norm=1.0707\n",
      "[iter 391] loss=1.5246 val_loss=0.0000 scale=1.0000 norm=1.0706\n",
      "[iter 392] loss=1.5240 val_loss=0.0000 scale=1.0000 norm=1.0705\n",
      "[iter 393] loss=1.5233 val_loss=0.0000 scale=1.0000 norm=1.0704\n",
      "[iter 394] loss=1.5227 val_loss=0.0000 scale=1.0000 norm=1.0703\n",
      "[iter 395] loss=1.5221 val_loss=0.0000 scale=1.0000 norm=1.0702\n",
      "[iter 396] loss=1.5216 val_loss=0.0000 scale=1.0000 norm=1.0701\n",
      "[iter 397] loss=1.5210 val_loss=0.0000 scale=1.0000 norm=1.0700\n",
      "[iter 398] loss=1.5204 val_loss=0.0000 scale=1.0000 norm=1.0698\n",
      "[iter 399] loss=1.5198 val_loss=0.0000 scale=1.0000 norm=1.0697\n",
      "[iter 400] loss=1.5192 val_loss=0.0000 scale=1.0000 norm=1.0697\n",
      "[iter 401] loss=1.5187 val_loss=0.0000 scale=1.0000 norm=1.0696\n",
      "[iter 402] loss=1.5181 val_loss=0.0000 scale=1.0000 norm=1.0695\n",
      "[iter 403] loss=1.5176 val_loss=0.0000 scale=1.0000 norm=1.0694\n",
      "[iter 404] loss=1.5170 val_loss=0.0000 scale=1.0000 norm=1.0693\n",
      "[iter 405] loss=1.5165 val_loss=0.0000 scale=1.0000 norm=1.0692\n",
      "[iter 406] loss=1.5160 val_loss=0.0000 scale=1.0000 norm=1.0691\n",
      "[iter 407] loss=1.5154 val_loss=0.0000 scale=1.0000 norm=1.0690\n",
      "[iter 408] loss=1.5149 val_loss=0.0000 scale=1.0000 norm=1.0689\n",
      "[iter 409] loss=1.5144 val_loss=0.0000 scale=1.0000 norm=1.0688\n",
      "[iter 410] loss=1.5139 val_loss=0.0000 scale=1.0000 norm=1.0687\n",
      "[iter 411] loss=1.5134 val_loss=0.0000 scale=1.0000 norm=1.0686\n",
      "[iter 412] loss=1.5129 val_loss=0.0000 scale=1.0000 norm=1.0686\n",
      "[iter 413] loss=1.5124 val_loss=0.0000 scale=1.0000 norm=1.0685\n",
      "[iter 414] loss=1.5119 val_loss=0.0000 scale=1.0000 norm=1.0684\n",
      "[iter 415] loss=1.5114 val_loss=0.0000 scale=1.0000 norm=1.0683\n",
      "[iter 416] loss=1.5109 val_loss=0.0000 scale=1.0000 norm=1.0682\n",
      "[iter 417] loss=1.5104 val_loss=0.0000 scale=1.0000 norm=1.0681\n",
      "[iter 418] loss=1.5099 val_loss=0.0000 scale=1.0000 norm=1.0680\n",
      "[iter 419] loss=1.5094 val_loss=0.0000 scale=1.0000 norm=1.0680\n",
      "[iter 420] loss=1.5090 val_loss=0.0000 scale=1.0000 norm=1.0679\n",
      "[iter 421] loss=1.5085 val_loss=0.0000 scale=1.0000 norm=1.0678\n",
      "[iter 422] loss=1.5081 val_loss=0.0000 scale=1.0000 norm=1.0678\n",
      "[iter 423] loss=1.5076 val_loss=0.0000 scale=1.0000 norm=1.0677\n",
      "[iter 424] loss=1.5072 val_loss=0.0000 scale=1.0000 norm=1.0676\n",
      "[iter 425] loss=1.5067 val_loss=0.0000 scale=1.0000 norm=1.0676\n",
      "[iter 426] loss=1.5063 val_loss=0.0000 scale=1.0000 norm=1.0675\n",
      "[iter 427] loss=1.5059 val_loss=0.0000 scale=1.0000 norm=1.0675\n",
      "[iter 428] loss=1.5055 val_loss=0.0000 scale=1.0000 norm=1.0674\n",
      "[iter 429] loss=1.5050 val_loss=0.0000 scale=1.0000 norm=1.0673\n",
      "[iter 430] loss=1.5046 val_loss=0.0000 scale=1.0000 norm=1.0673\n",
      "[iter 431] loss=1.5042 val_loss=0.0000 scale=1.0000 norm=1.0672\n",
      "[iter 432] loss=1.5038 val_loss=0.0000 scale=1.0000 norm=1.0672\n",
      "[iter 433] loss=1.5034 val_loss=0.0000 scale=1.0000 norm=1.0671\n",
      "[iter 434] loss=1.5030 val_loss=0.0000 scale=1.0000 norm=1.0671\n",
      "[iter 435] loss=1.5026 val_loss=0.0000 scale=1.0000 norm=1.0671\n",
      "[iter 436] loss=1.5022 val_loss=0.0000 scale=1.0000 norm=1.0670\n",
      "[iter 437] loss=1.5018 val_loss=0.0000 scale=1.0000 norm=1.0670\n",
      "[iter 438] loss=1.5014 val_loss=0.0000 scale=1.0000 norm=1.0669\n",
      "[iter 439] loss=1.5011 val_loss=0.0000 scale=1.0000 norm=1.0669\n",
      "[iter 440] loss=1.5007 val_loss=0.0000 scale=1.0000 norm=1.0669\n",
      "[iter 441] loss=1.5004 val_loss=0.0000 scale=1.0000 norm=1.0668\n",
      "[iter 442] loss=1.4999 val_loss=0.0000 scale=1.0000 norm=1.0667\n",
      "[iter 443] loss=1.4996 val_loss=0.0000 scale=1.0000 norm=1.0667\n",
      "[iter 444] loss=1.4992 val_loss=0.0000 scale=1.0000 norm=1.0666\n",
      "[iter 445] loss=1.4989 val_loss=0.0000 scale=1.0000 norm=1.0666\n",
      "[iter 446] loss=1.4985 val_loss=0.0000 scale=1.0000 norm=1.0665\n",
      "[iter 447] loss=1.4981 val_loss=0.0000 scale=1.0000 norm=1.0665\n",
      "[iter 448] loss=1.4978 val_loss=0.0000 scale=1.0000 norm=1.0664\n",
      "[iter 449] loss=1.4974 val_loss=0.0000 scale=1.0000 norm=1.0664\n",
      "[iter 450] loss=1.4971 val_loss=0.0000 scale=1.0000 norm=1.0663\n",
      "[iter 451] loss=1.4967 val_loss=0.0000 scale=1.0000 norm=1.0663\n",
      "[iter 452] loss=1.4964 val_loss=0.0000 scale=1.0000 norm=1.0663\n",
      "[iter 453] loss=1.4960 val_loss=0.0000 scale=1.0000 norm=1.0662\n",
      "[iter 454] loss=1.4957 val_loss=0.0000 scale=1.0000 norm=1.0662\n",
      "[iter 455] loss=1.4954 val_loss=0.0000 scale=1.0000 norm=1.0661\n",
      "[iter 456] loss=1.4950 val_loss=0.0000 scale=1.0000 norm=1.0661\n",
      "[iter 457] loss=1.4947 val_loss=0.0000 scale=1.0000 norm=1.0661\n",
      "[iter 458] loss=1.4944 val_loss=0.0000 scale=1.0000 norm=1.0660\n",
      "[iter 459] loss=1.4941 val_loss=0.0000 scale=1.0000 norm=1.0660\n",
      "[iter 460] loss=1.4938 val_loss=0.0000 scale=1.0000 norm=1.0660\n",
      "[iter 461] loss=1.4934 val_loss=0.0000 scale=1.0000 norm=1.0659\n",
      "[iter 462] loss=1.4931 val_loss=0.0000 scale=1.0000 norm=1.0659\n",
      "[iter 463] loss=1.4928 val_loss=0.0000 scale=1.0000 norm=1.0658\n",
      "[iter 464] loss=1.4925 val_loss=0.0000 scale=1.0000 norm=1.0658\n",
      "[iter 465] loss=1.4922 val_loss=0.0000 scale=1.0000 norm=1.0658\n",
      "[iter 466] loss=1.4919 val_loss=0.0000 scale=1.0000 norm=1.0658\n",
      "[iter 467] loss=1.4917 val_loss=0.0000 scale=1.0000 norm=1.0657\n",
      "[iter 468] loss=1.4914 val_loss=0.0000 scale=1.0000 norm=1.0657\n",
      "[iter 469] loss=1.4911 val_loss=0.0000 scale=1.0000 norm=1.0657\n",
      "[iter 470] loss=1.4908 val_loss=0.0000 scale=1.0000 norm=1.0657\n",
      "[iter 471] loss=1.4905 val_loss=0.0000 scale=1.0000 norm=1.0656\n",
      "[iter 472] loss=1.4903 val_loss=0.0000 scale=1.0000 norm=1.0656\n",
      "[iter 473] loss=1.4900 val_loss=0.0000 scale=1.0000 norm=1.0656\n",
      "[iter 474] loss=1.4897 val_loss=0.0000 scale=1.0000 norm=1.0656\n",
      "[iter 475] loss=1.4895 val_loss=0.0000 scale=1.0000 norm=1.0656\n",
      "[iter 476] loss=1.4892 val_loss=0.0000 scale=1.0000 norm=1.0655\n",
      "[iter 477] loss=1.4889 val_loss=0.0000 scale=1.0000 norm=1.0655\n",
      "[iter 478] loss=1.4887 val_loss=0.0000 scale=1.0000 norm=1.0655\n",
      "[iter 479] loss=1.4884 val_loss=0.0000 scale=1.0000 norm=1.0655\n",
      "[iter 480] loss=1.4882 val_loss=0.0000 scale=1.0000 norm=1.0655\n",
      "[iter 481] loss=1.4879 val_loss=0.0000 scale=1.0000 norm=1.0655\n",
      "[iter 482] loss=1.4877 val_loss=0.0000 scale=1.0000 norm=1.0655\n",
      "[iter 483] loss=1.4875 val_loss=0.0000 scale=1.0000 norm=1.0654\n",
      "[iter 484] loss=1.4872 val_loss=0.0000 scale=1.0000 norm=1.0654\n",
      "[iter 485] loss=1.4870 val_loss=0.0000 scale=1.0000 norm=1.0654\n",
      "[iter 486] loss=1.4868 val_loss=0.0000 scale=1.0000 norm=1.0654\n",
      "[iter 487] loss=1.4865 val_loss=0.0000 scale=1.0000 norm=1.0654\n",
      "[iter 488] loss=1.4863 val_loss=0.0000 scale=1.0000 norm=1.0653\n",
      "[iter 489] loss=1.4860 val_loss=0.0000 scale=1.0000 norm=1.0653\n",
      "[iter 490] loss=1.4858 val_loss=0.0000 scale=1.0000 norm=1.0653\n",
      "[iter 491] loss=1.4856 val_loss=0.0000 scale=1.0000 norm=1.0653\n",
      "[iter 492] loss=1.4853 val_loss=0.0000 scale=1.0000 norm=1.0652\n",
      "[iter 493] loss=1.4851 val_loss=0.0000 scale=1.0000 norm=1.0652\n",
      "[iter 494] loss=1.4848 val_loss=0.0000 scale=1.0000 norm=1.0652\n",
      "[iter 495] loss=1.4846 val_loss=0.0000 scale=1.0000 norm=1.0652\n",
      "[iter 496] loss=1.4844 val_loss=0.0000 scale=1.0000 norm=1.0651\n",
      "[iter 497] loss=1.4842 val_loss=0.0000 scale=1.0000 norm=1.0651\n",
      "[iter 498] loss=1.4840 val_loss=0.0000 scale=1.0000 norm=1.0651\n",
      "[iter 499] loss=1.4837 val_loss=0.0000 scale=1.0000 norm=1.0651\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-01c35a458269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mngb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mY_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mY_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/ngboost/ngboost.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/ngboost/ngboost.py\u001b[0m in \u001b[0;36mpred_dist\u001b[0;34m(self, X, max_iter)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpred_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/ngboost/ngboost.py\u001b[0m in \u001b[0;36mpred_param\u001b[0;34m(self, X, max_iter)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mresids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mresids\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/ngboost/ngboost.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mresids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mresids\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/ngboost/esn_learners.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRidge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/simple_esn/simple_esn.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             curr_ = (1-self.damping)*curr_ + self.damping*tanh(\n\u001b[0;32m--> 192\u001b[0;31m                 self.input_weights_.dot(u) + self.weights_.dot(curr_))\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ngboost.learners import esn_ridge_learner\n",
    "model_test(Base=default_tree_learner,\n",
    "           X_train=X_train[:8000], X_test=X_test,\n",
    "           Y_train=Y_train[:8000], Y_test=Y_test,\n",
    "          n_estimators=500, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222.60870361328125px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
